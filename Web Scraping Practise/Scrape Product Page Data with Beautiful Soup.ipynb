{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import simplejson as json\n",
    "import ast\n",
    "import csv\n",
    "import pandas as pd\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "array_of_urls = [\"/v-by-very-embroidered-lace-insert-skater-dress/1600177942.prd\",\n",
    "       \"/v-fit-folding-x-frame-cycle-exercise-bike/1148569588.prd\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "array_of_product_information = []\n",
    "with open('product_urls.csv', 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        url = line.strip().split(',')\n",
    "        array_of_product_information.append((url[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "array_of_urls = []\n",
    "for x in array_of_product_information[1:]:\n",
    "    array_of_urls.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def request_web_data(url, brand_domain):\n",
    "    #Function that handles the request to get the web page. \n",
    "    #Consider adding in a sleep at this point if crawl \n",
    "    #requests get a bit to much.\n",
    "    \n",
    "    r  = requests.get(brand_domain + url)\n",
    "    web_page_data = r.text\n",
    "    \n",
    "    return web_page_data\n",
    "\n",
    "\n",
    "def parse_web_data(web_page_data):\n",
    "    #Parses and cleans the scraped data. If any more \n",
    "    #data is needed, then get it from the soup object\n",
    "    \n",
    "    soup = BeautifulSoup(web_page_data, \"html.parser\")\n",
    "    description = soup.find(itemprop=\"description\")\n",
    "    \n",
    "    try:\n",
    "        h2 = description.h2.get_text()\n",
    "        description.h2.string = \"\"\n",
    "        header = 'yes'\n",
    "    except:\n",
    "        h2 = ''\n",
    "        header = 'no'\n",
    "   \n",
    "    try:\n",
    "        text = description.get_text(strip='\\n')\n",
    "        content = 'yes'\n",
    "    except:        \n",
    "        text = ''\n",
    "        content = 'no'\n",
    "    \n",
    "    try:\n",
    "        sku = soup.find('input', attrs={\"class\":\"speedtrapProductID\"})\n",
    "        sku = sku['value']\n",
    "        sku_found = 'yes'\n",
    "    except:\n",
    "        sku = ''\n",
    "        sku_found = 'no'\n",
    "    \n",
    "    data_dictionary = {'sku':sku, 'sku_found': sku_found, 'h2': h2, 'header':header, 'text': text , 'content': content}\n",
    "    \n",
    "    return data_dictionary\n",
    "\n",
    "def make_data_frame(array_of_urls):\n",
    "    #Runs through an array of URL and outputs a dataframe\n",
    "    #at the end. \n",
    "    \n",
    "    result = []\n",
    "    df = pd.DataFrame(columns=['sku', 'sku_found', 'h2', 'header', 'text', 'content'])\n",
    "    for url in array_of_urls:\n",
    "        data = request_web_data(url ,\"http://www.very.co.uk\")\n",
    "        result.append(parse_web_data(data))\n",
    "        \n",
    "    df  = df.append(result, ignore_index=True)\n",
    "        \n",
    "    return df\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_field = make_data_frame(array_of_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_field['word count'] = data_field['text'].str.split().apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_field.to_csv('string_count.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for url in array_of_urls:\n",
    "    print(array_of_urls[url])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "array_of_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
